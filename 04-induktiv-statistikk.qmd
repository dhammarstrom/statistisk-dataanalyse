---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Statistisk inferens {#sec-statistiskinferens}

Vi mennesker er gode på å trekke slutninger om hvordan verden fungerer basert på et begrenset antall observasjoner. Denne evne til å lage mentale modeller av verden kan sies være et særtrekk i mennesket som gjort det mulig å skape avanserte sivilisasjoner. En mental modell, eller forståelse av hvordan verden henger sammen gir grunnlag for å samhandle med den, og forandre den. Noen ganger går det dessverre galt, vår mentale modell representerer ikke verden og utfall blir ikke hva vi forventer.

Innen vitenskapen prøver vi å systematisere prosessen som leder frem til ny kunnskap. Flere forskjellige filosofier blir brukt for å unngå å trekke falske slutninger om verden basert på data. Den filosofiske og statistiske skole som trolig blir mest brukt innen vitenskapen i dag kalles for frekventisme. Denne modulen vil introdusere statistisk inferens med fokus på frekventisme. Statistisk inferens er det å trekke konklusjoner om en populasjon basert på et utvalg. Vi ønsker å si noe om noe vi ikke observerer, basert på et begrenset datautvalg.

## Populasjon, utvalg og målet med statistisk inferens
En populasjon i statistikken er som nevnes i @thrane_2020 en samling av alle mulige observasjoner med et sett med spesifikke karakteristikker. Denne definisjonen brukes på litt forskjellige måter, men for at den skal være av betydning i vår videre diskusjon bør den si noe om hva vi ønsker å måle og i hvilken kontekst. Kanskje er vi interesserte i IQ (*hva*) hos menn og kvinner mellom 18 og 65 år i Norge (*kontekst*). Vi har ikke mulighet å undersøke hele populasjonen, men et lite utvalg. Målet med å undersøke et utvalg er å si noe om populasjonen. I utvalget kan vi beregne noen deskriptive statistikker som gjennomsnitt og spredning. Samtidig som dette sier noe om dataene som vi har er det også et *estimat* av parametere i populasjonen. I den enkleste forståelsen av begrepet modell, kan gjennomsnitt og spredning fungere som en modell av populasjonen. Basert på disse kan vi trekke slutninger om populasjonen.

::: {.column-margin}
En *parameter* er en kvantitativ egenskap hos en *populasjon*. En *populasjon* er i sin tur en samling av mulige verdier med et sett av gitte egenskaper. En parameter hos populasjonen kan være dess gjennomsitt (iblant kalt $\mu$) eller standardavvik ($\sigma$). Når vi estimerer gjennsomnitt og standardavvik i et utvalg gir vi disse kvantiteterne andra symboler, $\bar{x}$ og $s$. Disse er estimater av populasjonsgjennomsnittet og standardavviket [@dodge_concise_2008].
:::


### Utvalg og generalisering
For å trekke korrekte slutninger om en populasjon kreves at utvalget er representativt for populasjonen. Når utvalget representerer den populasjon man ønsker å undersøke kan man gjøre den generalisering som det innebærer å trekke konklusjoner om populasjonen basert på utvalget. Ideelt sett trekkes et utvalg fra populasjonen helt tilfeldig. Dette gir en garanti mot at utvalget ikke skiller seg fra populasjonen i noen viktige karakteristikker. I forskningen er dette i praksis veldig vanskelig.

Tenkt deg at du ønsker å studere effekten av trening i den voksne norske befolkningen, vi ønsker å generalisere resultater fra studien til hele befolkningen, menn, kvinner, unge, gamle, friske og individer som sliter med noen helseplager. Vi går ut i lokalavisen å sier at vi gjennomfører en studie som bruker høyintensiv trening for å forbedre fysisk prestasjonsevne. Interesserte kan melde seg til studien ved å ringe eller sende en e-post. Denne rekrutteringen vil introdusere en karakteristikk i utvalget som ikke kan sies representere populasjonen, dette da individer som ønsker å gjennomføre høyintensiv trening melder seg til studien.

Hvis vi prøver å gjøre noe åt dette kan vi sende ut et påmeldingsskjema til la oss si 1000 privatadresser i Lillehammer. Vi vil fortsatt sitte igjen med et utvalg som ikke representerer populasjonen, men vi har nå mulighet å undersøke de som ikke melder seg på. Vi kan spørre de som ikke er interesserte i å delta hvorfor det er slik, dette kan si noe om hva utvalget representerer og hvor langt vi kan generalisere resultater fra studien. 

I praksis er ulike former av bekvemmelighetsutvalg trolig den vanligste formen for utvalg i mye av forskningen. Med bekvemmelighetsutvalg mener vi et utvalg som vi har tilgang til. En vel undersøkt populasjon innen fysiologisk idrettsforskning er mannlige studenter ved idrettsutdanninger.



## Utvalg og estimering
Når vi har et utvalg så kan vi måle noe og dermed estimere den *sanne verdien*[^1] i populasjonen. Da vi ønsker å si noe om *den sanne verdien* sier dette også noe om at vi kan være mer eller mindre sikre på et estimat, og vi kan ha feil. Vi må ha verktøy som tar hensyn til begge disse konseptene som er tett sammenkoblet nemlig, presisjon og feilrate. Vi kan starte med å konstatere at all estimering gjøres med usikkerhet, men hvordan kan vi si noe om usikkerheten. Vi vil nå gjennomføre et tankeeksperiment.


[^1]: I frekventisme ser vi på populasjonsparameteren som en (teoretisk) gitt verdi som ikke forandres.


I frekventisme er det mulig å tenke seg at vi i teorien kan trekke flere uavhengige utvalg fra en populasjon. La oss gjøre dette, vi trekker flere utvalg med størrelse 10 (10 observasjoner). Fra hvert utvalg kan vi beregne gjennomsnitt og standardavvik. Vi legger sammen gjennomsnittene fra de mange utvalgene i en ny fordeling, en fordeling av gjennomsnitt basert på utvalg fra en populasjon. Det viser seg at en fordeling av gjennomsnitt har det samme gjennomsnittet som populasjonen og at spredningen (standardavviket) i denne fordelingen bestemmes av størrelsen på utvalgene. Standardfeilen er spredningen i en fordeling av gjennomsnitt fra flere utvalg. Standardfeilen (SE, standard error på engelsk) beregnes som

$$SE = \frac{\sigma}{\sqrt{n}}$$
hvor $\sigma$ er standardavviket i populasjonen og $n$ er størrelsen på utvalget. Problemet her er at vi ikke kjenner $\sigma$, isteden vil vi bruke det estimerte standardavviket fra et utvalg for å gjennomføre beregning. 

$$SE = \frac{s}{\sqrt{n}}$$
Det viser seg at når vi trekker flere utvalg så vil vi i det lange løp, i gjennomsnitt, få standardfeil i utvalgene som tilsvarer standardavviket i utvalgsfordelingen. Dette er et fantastisk resultat, og grunnen til at vi kan si noe om populasjonen basert på et utvalg.

### Estimere et gjennomsnitt, et eksempel

I Norge 2022 ble 52026 fødsler registrert i [Medisinsk fødselsregister](https://statistikkbank.fhi.no/mfr/). Gjennomsnittet for fødselsvekt var 3485 gram og standardavviket var 587 gram. Vi kan si at vi dermed kjenner til disse egenskapene i populasjonen, men hvor godt hadde vi klart å estimere disse verdiene hvis vi hadde trukket et utvalg på 10 barn fra populasjonen. For å besvare det spørsmålet kan vi lage et eksperiment hvor vi trekker 1000 utvalg fra populasjonen og beregner gjennomsnitt i hvert utvalg. Den resulterende utvalgfordelingen vil gi et bilde av hvor godt vi kan estimere populasjonen basert på et utvalg. Som vi ser i @fig-birtwtutvalg, i panelet med utvalgsfordeling kan et gjennomsnitt forventes være så lite som mindre enn 3000 og så stort som større enn 4000 g. Hvis utvalgsstørrelsen istedenfor 10 hadde vært 100, ville vi sett at utvalgsfordelingen var mer samlet rundt gjennomsnittet. 

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-width: 8
#| label: fig-birtwtutvalg
#| fig-height: 8
#| fig-cap: "Vi trekker utvalg (n = 10) fra en populasjon av fødselsvekter. For hvert utvalg beregner vi gjennomsnitt og legger hvert gjennomsnitt til en ny fordeling, utvalgsfordelingen. I figuren ser vi tre eksempel på utvalg og de totalt tusen utvalgen som i dette fallet skaper utvalgsfordelingen. Utvalgsfordelingen tar en form som kan beskrives som en symmetrisk fordeling. Hvis vi fordelingen fra populasjonen med en utvalgsfordeling med forskjellige utvalgsstørrelser ser vi at populasjonen er har størst spredning og en utvalgsfordeling med flere observasjoner er mer samlet kring gjennomsnittet."


library(tidyverse); library(ggtext); library(cowplot)


px <- data.frame(x = c(0,1), y = c(0,1)) %>%
  

  
  ggplot(aes(x, y)) + 
  
    scale_x_continuous(limits = c(0, 1), breaks = NULL) +
  
  scale_y_continuous(limits = c(0, 1), breaks = NULL) +
  theme_void()


p1 <- ggplot(data = data.frame(x = c(3485 - (4*587), 3485 + (4*587))), aes(x)) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = 3485, sd = 587)) + 
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
    labs(x = "F&#248;dselsvekt (gram)", 
         title = "Populasjonen") +
  theme(panel.grid = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), 
        axis.title.x = element_markdown()) 


set.seed(1)
## data sets
d1 <- data.frame(x = rnorm(10, 3485, 587))
d2 <- data.frame(x = rnorm(10, 3485, 587))
d3 <- data.frame(x = rnorm(10, 3485, 587))



samp_plot_fun <- function(d = d1) {
  
  d %>%
  ggplot(aes(x)) + 
  geom_dotplot(fill = "lightblue") +
  
  annotate("text", x = mean(d$x), y = 0.6, 
           label = "Gjennomsnitt", 
           vjust = 0, 
           hjust = 0.5, size = 4) +  
  
  annotate("segment", x = mean(d$x), xend = mean(d$x), 
  y = 0.55, yend = 0.2, size = 0.5, 
  arrow = arrow(type = "closed", length = unit(0.1, "inches"))) +  
      
  
  ylab("") +
  labs(x = "F&#248;dselsvekt (gram)") +
    scale_x_continuous(limits = c(3485 - (4*587), 3485 + (4*587)), breaks = c(1000, 3500, 6000)) +
    scale_y_continuous(breaks = NULL, limits = c(0, 0.7)) +
  theme(panel.grid = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        plot.background = element_rect(fill = NULL))
  
  
}


p2 <- samp_plot_fun(d1)
p3 <- samp_plot_fun(d2)
p4 <- samp_plot_fun(d3)


## Sample distribution plot


dsamp1 <- bind_rows(d1  %>% summarise(x = mean(x))%>% mutate(samp = "s1"), 
          d2  %>% summarise(x = mean(x))%>% mutate(samp = "s2"), 
          d3  %>% summarise(x = mean(x))%>% mutate(samp = "s3"))


p5 <- data.frame(x = rnorm(1000, 3485, 587/sqrt(10)), 
           samp = "all") %>%
  ggplot(aes(x, fill = factor(samp))) +

  
  geom_dotplot(binwidth = 25,
               dotsize = 2.4,
              stackratio = 1) +
  
  geom_dotplot(data = dsamp1, aes(x = x,fill = samp), 
               binwidth = 25, 
               stackratio = 1, 
               dotsize = 5.5) +
  

    scale_fill_manual(values = c("lightblue", "steelblue", "orchid", "green")) +
  
  ylab("") +
    scale_x_continuous(limits = c(3485 - (4*587), 3485 + (4*587))) +
  theme(panel.grid = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        plot.background = element_rect(fill = NULL), 
        legend.position = "none") + 
  labs(title = "Utvalgsfordeling")


## Combine population and sample plots

p6 <- ggplot(data = data.frame(x = c(3485 - (4*587), 3485 + (4*587))), aes(x)) +
  # Population
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = 3485, sd = 587)) + 
  # Sampling distribution n = 10
    stat_function(fun = dnorm, n = 101, 
                  color = "steelblue",
                args = list(mean = 3485, sd = 587/sqrt(10))) + 
  # Sampling distribution n = 100
      stat_function(fun = dnorm, n = 101, 
                  color = "orchid",
                args = list(mean = 3485, sd = 587/sqrt(100))) + 
  
  
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
    labs(x = "F&#248;dselsvekt (gram)", 
         title = "Populasjonen og utvalgsfordelinger") +
  theme(panel.grid = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), 
        axis.title.x = element_markdown()) +
  annotate("text", 
           x = 1000, 
           hjust = 0,
           size = 4,
           y = c(0.001,0.0015, 0.002),
           color = c("black", "steelblue", "orchid"),
           label = c("Populasjon",
                     "Utvalg, n = 10", 
                     "Utvalg, n = 100")) 



px + 
  draw_plot(p1, x = 0.01, y = 0.6, width = 0.35, height = 0.4) +
  draw_plot(p2, x = 0.05, y = 0.41, width = 0.2, height = 0.2) +
  draw_plot(p3, x = 0.05, y = 0.21, width = 0.2, height = 0.2) +
  draw_plot(p4, x = 0.05, y = 0.01, width = 0.2, height = 0.2) +
  
  
  annotate("text", 
           x = 0.01, 
           hjust = 1,
           y = 0.61, 
           label = "n = 10") +
  
  annotate(
    'curve',
    x = c(0.03,0.03, 0.03), 
    y = c(0.62,0.62, 0.62),
    yend = c(0.51,0.31, 0.11),
    xend = c(0.06,0.06, 0.06),
    linewidth = 0.8,
    curvature = 0.3,
    arrow = arrow(length = unit(0.25, 'cm'), type = "closed")
  )  +
  
    annotate(
    'curve',
    x = c(0.25,0.25, 0.25), 
    y = c(0.51,0.31, 0.11),
    yend = c(0.31,0.30, 0.29),
    xend = c(0.35,0.35, 0.35),
    linewidth = 0.8,
    curvature = 0.1,
    color = c("steelblue", "orchid", "green"),
    arrow = arrow(length = unit(0.25, 'cm'), type = "closed")
  ) +
  
  annotate("richtext", 
           x = 0.35, 
           hjust = 1,
           fill = NA, label.color = NA, # remove background and outline
           label.padding = grid::unit(rep(0, 4), "pt"), # remove padding
           y = 0.12, 
           label = "&times;1000") +
  
  
  draw_plot(p5, x = 0.36, y = 0.05, width = 0.4, height = 0.45) +
  
  draw_plot(p6, x = 0.58, y = 0.5, width = 0.45, height = 0.55) +
  
  annotate(
    'curve',
    x = c(0.81), 
    y = c(0.31),
    yend = c(0.5),
    xend = c(0.85),
    linewidth = 0.8,
    curvature = 0.1,
    arrow = arrow(length = unit(0.25, 'cm'), type = "closed")
  ) 



```

Hva er da poenget med dette? I den frekventistiske statistikken tenker vi oss at vi gjør dette eksperimentet hver gang vi skal estimere en populasjonsparameter. Usikkerheten i estimatet representeres av spredningen i utvalgsfordelingen. Men i praksis gjør vi jo ikke dette eksperimentet, isteden estimerer vi en populasjonsparameter som gjennomsnittet en gang og bruker spredningen i utvalget for å også estimerer spredningen i utvalgsfordelingen.


## Estimering av spredningen i utvalgsfordelingen

Som vi kan se over i beregningen av standardfeilen så er den avhengig av utvalgsstørrelsen. Når utvalgsstørrelsen ($n$) er større blir standardfeilen mindre. Det betyr at fordelingen av gjennomsnitt fra utvalgene vil være tettere samlet kring den *sanne verdien*, populasjonsgjennomsnittet, når utvalgsstørrelsen er større. Vi så dette i @fig-birtwtutvalg.

En annen observasjon som kan gjøres av utvalgsfordelingen er at den vil ha en lignende form uansett underliggende populasjonsfordeling. Fordelingen vil ligne på det som kalles normalfordeling. Normalfordelingen bestemmes av et gjennomsnitt og et standardavvik. Dette betyr at vi i mange tilfeller kan bruke estimerte gjennomsnitt og standardavvik for å lage en modell av utvalgsfordelingen. 

::: {.column-margin}

For enda bedre presisjon i estimeringen av en utvalgsfordeling når utvalgsstørrelsen er liten brukes en $t$-fordeling. Denne fordelingen tar også hensyn til utvalgsstørrelsen. Da utvalgsfordelingen har kjente egenskaper (normalfordelingen og $t$-fordelingen) så kan vi bruke denne for å si noe om hvordan vi ser for oss at en teoretisk fordeling av flere gjennomsnitt ser ut.

:::

Men hvor sikre kan vi være på estimatet av spredningen i en utvalgsfordeling? Spredningen i utvalgsfordelingen er altså spredningen av for eksempel gjennomsnitt hvis vi hadde trukket flere utvalg fra den samme populasjonen og beregnet gjennomsnitt for hvert av dem. Vi estimerer denne spredningen ved å beregne standardfeilen (SE). Hvis vi bruker standardavviket i hvert utvalg som et estimat for standardavviket i populasjonen, så kan vi også beregne standardavviket i utvalgsfordelingen. Dette gjør vi, som vi allerede har sett ved å dele standardavviket i populasjonen på kvadratroten av utvalgsstørrelsen. Hvis vi så hadde gjort dette som et eksperimentet hvor vi beregner standardfeilen mange ganger, så ville vi kunne se at vi i gjennomsnitt, i de fleste tilfeller være veldig nærme den faktiske variasjonen (se @fig-utvalgse).




```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-height: 4.5
#| fig-width: 4
#| label: fig-utvalgse
#| fig-cap: "Fordeling av estimat av standardfeil fra en populasjon med gjennomsnitt 3485 og standardavvik 587 og utvalg med størrelse 10. Den röde streken indikerer den teoretiske standardfeilen bergnet fra populasjonen."


dat <- vector()

set.seed(12)

for(i in 1:1000) {
  
  dat[i] <-  sd(rnorm(10, 3485, 587)) / sqrt(10) 
  
  
  
}


data.frame(x = dat) %>%
  ggplot(aes(x)) + geom_dotplot(dotsize = 1, 
                                binwidth = 5, 
                                fill = "lightblue") + 
  geom_vline(xintercept = 587/sqrt(10), color = "red") +
  theme_classic() + labs(x = "Standardfeil") + 
  theme(axis.title.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.line.y = element_blank(), 
        axis.text.y = element_blank())






```

## Målet og problemet med statistisk inferens

Så hva er problemet? Vi kan altså estimerer verdier i populasjonen ved hjelp av utvalg, og er vi usikre så kan vi alltid lage et eksperiment hvor vi trekker flere utvalg? I praksis har vi bare et begrenset utvalg, vi har veldig sjelden flere enn et utvalg og aldri 1000. Vi må altså stole på at estimatene vi skaper ved hjelp av et utvalg gir en god representasjon av populasjonen. Men hvordan kan vi vite når vi faktisk har rett?

## Konfidensintervaller
Et konfidensintervall tar utgangspunkt i den estimerte utvalgsfordelingen. Basert på utvalgsfordelingen lager vi et intervall som fanger inn en gitt prosent av alle mulige gjennomsnitt fra en teoretisk samling av utvalg. Av tradisjon brukes ofte et 95% intervall. Et 95% intervall gir oss et intervall av gjennomsnittsverdier som inneholder 95% av alle utvalg ved en (teoretisk) repeterte utvalgsprosess. Dette sier også noe om definisjonen av konfidensintervallet. Ved repeterte utvalg inneholder konfidensintervallene populasjonsparameteren (for eksempel gjennomsnittet) i 95% av tilfellene. Dessverre vet vi ikke om et spesifikt intervall gjør det eller ikke. 

::: {.column-margin}
Man kan argumentere for at definisjonen i [@thrane_2020, sid. 92] gir en feilaktig bilde av konfidensintervallet. Det er altså ikke slik at et konfidensintervall i seg har en sikkerhet. Et enkelt intervall inneholder populasjonsparameteren, eller så inneholder det ikke parameteren. Prosenttallet som vi setter på intervallet sier noe om prosessen med repeterte utvalg. Det sier noe om hvor ofte vi tar feil ved repeterte utvalg fra den samme populasjonen.
:::

Hvis vi forandrer frekvensen med hvilken vi kan ha feil fra 5% (95% konfidensintervall) til 10% (90% konfidensintervall) vil intervallet bli mindre. Altså med en større risk at enkelte konfidensintervall ikke inneholder populasjonsparameteren får vi et intervall som bedre beskriver populasjonsparameteren (hvis vi har rett konfidensintervall). Vi kan gå andre veien også, et 99% konfidensintervall er et intervall som holder flere teoretiske verdier som mulige for populasjonsparameteren, dette intervallet kommer fra en samling intervaller hvor bare 1 av 100 ikke finner den sanne verdien. Igjen, vi vet ikke hvis vi har et intervall som er rett eller galt.

Utvalgsstørrelsen vil påvirke bredden på intervallene, men ved repeterte utvalg vil vi til tross av dette ha feil i en gitt andel av tilfellene.

### Beregne et konfidensintervall

For å beregne et konfidensintervall trenger vi et gjennomsnitt med tilhørende standardfeil og en funksjon som beskriver en sannsynlighetsfordeling. Vi har allerede snakket om normalfordelingen, dette er et eksempel på en sannsynlighetsfordeling. Normalfordelingen er en symmetrisk fordeling som beskrives av to parametere, gjennomsnitt og standardavvik. Vi er interessert i å bruke normalfordelingen for å skape et intervall som inkluderer, la oss si, 95% av alle mulige verdier, gitt at vi har et gjennomsnitt og en spredning (standardfeilen). 

$$\bar{x} \pm z_{\alpha/2} \times \frac{s}{\sqrt{n}}$$
I formelen over er $\bar{x}$ gjennomsnittet, $s$ standardavviket, $n$ antall observasjoner og $z_{\alpha/2}$ er kvantilen vi ønsker til en normalfordeling og den tilsvarende faktoren vi trenger for å fange denne kvantilen. For et 95% konfidensintervall er $z_{\alpha/2} = 1.96$. $\alpha$ er den parameter som bestemmer sannsynligheten for å gjøre feilen at ikke fange populasjonsparameteren ved repeterte utvalg. For et 95% konfidensintervall er $\alpha = 0.05$ og $\alpha/2 = 0.025$. $\alpha/2 = 0.025$ betyr i sin tur at vi lar 2.5% av sannsynlighetsmassen i endene av fordelingen (halene) representere tilfellene hvor vi aksepterer å ha feil. 

```{r}
#| echo: false
#| message: FALSE
#| warning: FALSE

set.seed(1)
sample <- rnorm(10,3485,587) 

```


Vi trekker et utvalg med størrelse 10 fra populasjonen av registrerte fødselvekter i Norge 2022. Tallene er:

```{r}
#| echo: false
#| message: FALSE
#| warning: FALSE


library(gt)

data.frame(Vekt = round(sample, 0)) %>%
  gt() %>%
  tab_header(title = md("F&#248;dselsvekter i Norge 2022") )
```




Gjennomsnittet er `r round(mean(sample),0)` og standardavviket er `r round(sd(sample),0)`. Vi kan bruke disse verdiene til å beregne konfidensintervallet.

$$3563 \pm 1.96 \times \frac{485}{\sqrt{10}}$$
hvilket gir et intervall fra `r round(round(mean(sample),0) -  1.96 * (round(sd(sample),0)/sqrt(10)),0)`  til `r round(round(mean(sample),0) +  1.96 * (round(sd(sample),0)/sqrt(10)),0)` gram.

I tekst kan vi sammenfatte våre beregninger som:

> Gjennomsnittet av fødselvektene i Norge 2022 er estimert til `r round(mean(sample),0)` gram med et 95% konfidensintervall på [`r round(round(mean(sample),0) -  1.96 * (round(sd(sample),0)/sqrt(10)),0)`, `r round(round(mean(sample),0) +  1.96 * (round(sd(sample),0)/sqrt(10)),0)`] gram.

### *t*-fordelingen

Når vi bruker små utvalg er det bedre å bruke en *t*-fordelning når vi lager konfidensintervaller. En *t*-fordeling er en familie av fordelinger som likt normalfordelingen er symmetriske med tyngdepunkt ved sentrum. Formen på fordelingen bestemmes av antallet frihetsgrader, noe som i sin tur bestemmes av antallet observasjoner. Når antall frihetsgrader er lavt vil fordelingen være bredere og ha mer masse lengre ut fra sentrum. Når antallet frihetsgrader øker vil fordelingen nærme seg en normalfordeling (se @fig-konfidensintervall). Når vi beregner et konfidensintervall for et gjennomsnitt bruker vi $n-1$ frihetsgrader. I eksemplet med 10 observasjoner vil vi bruke en *t*-fordeling med 9 frihetsgrader.

Det at vi fanger inn mer av fordelingen lengre ut fra sentrum gjør at vi kan være mer sikre på at vi unngår å lure oss selve hva gjelder populasjonsparameteren. Hvis vi definerer feilraten som antallet 95% konfidensintervaller som ikke fanger inn populasjonsparameteren, vil vi ved bruk av en *t*-fordeling ha en feilrate som ikke overstiger 5% ved repeterte forsøk, 95% av konfidensintervallene vil faktisk fange populasjonsparameteren. Normalfordelingen og små utvalg vil derimot gi oss 95% konfidensintervall som ikke holder hva de lover, en lavere andel enn 95% vil inneholde populasjonsparameteren. I @fig-konfidensintervall C har viser vi resultatet av 50 simulerte utvalg fra fødselsvekt-dataene. Over hvert panel i figuren angis feilraten fra 2000 simuleringer. Når vi har små utvalg (n = 10) vil feilraten være større når vi bruker normalfordelingen som grunn for konfidensintervallene.

```{r}
#| echo: false
#| message: FALSE
#| warning: FALSE
#| fig-width: 6
#| fig-height: 8
#| label: fig-konfidensintervall
#| fig-cap: !expr 'paste("(A) En normalfordeling skiller seg fra en t-fordeling ved at formen ikke kan påvirkes. t-fordelingen har en ekstra parameter frihetsgrader (degrees of freedom, df) som bestemmer formen på kurven, når df nermer seg 0 faller mer av massen i fordelingen lengre ut fra sentrum. Når df > 30 ligner t-fordelingen på en normalfordeling. Vi betsemmer antallet frihetsgrader basert på antall observasjoner i dataene. (B) Et konfidensintervall basert på en t-fordeling (df=9) for observert data, et 95% konfidenintervall strekker seg &pm; ",round(qt(0.975, df=9),2)," standardfeil ut fra gjennomsnittet. (C) Et konfidensintervall for en t-fordeling er bredere enn for en normalfordeling og når antallet frihetsgrader (antallet observasjoner) er litet vil dette hjelpe oss å opprettholde feilraten, antallet konfidensintervall som ikke inneholder den faktiske populasjonsparameteren. Et antall intervall i figuren finner ikke populasjonsparameteren, feilraten er angitt over hvert panel og intervaller er markert i figuren")'



# plot a normal distribution using ggplot2

p1 <- data.frame(x = c(-5, 5)) %>%
  ggplot(aes(x)) + 
  stat_function(fun = dnorm, args = list()) +
  stat_function(fun = dt, args = list(
                                      df = 30), 
                color = "orchid") +
    stat_function(fun = dt, args = list(
                                      df = 9), 
                color = "blue") +
    stat_function(fun = dt, args = list(
                                      df = 2), 
                color = "orange") +
  theme_classic() + 
  
  annotate("text", 
           x = -4.9, 
           y = c(0.35,
                 0.3,
                 0.25,
                 0.20),
           size = 4,
           hjust = 0,
           label = c("Normal", 
                     "t, df = 30", 
                     "t, df = 9",
                     "t, df = 2"), 
                     color = c("black", "orchid", 
                               "blue", "orange")) +
  
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(), 
        axis.line.y = element_blank(), 
        axis.title.x = element_blank()) +
  labs(title = "Normal og t-fordelning",
       subtitle = " ",
       x = "Value") 


## Shade are under a t distribution

# Return dt(x) for 0 < x < 2, and NA for all other x

# example data

samp <- rnorm(10)

samp <- data.frame(samp = (samp - mean(samp))/sd(samp))


dt_limit <- function(x) {
    y <- dt(x, df = 10)
    y[x < -qt(0.975, df = 9)  |  x > qt(0.975, df = 9)] <- NA
    return(y)
}

# ggplot() with dummy data
p2 <- ggplot(data.frame(x = c(-5, 5)), aes(x = x)) +
  stat_function(fun = dt_limit, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dt, 
                args = list(df = 9), color = "blue") + 
  theme_classic() +
    theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(), 
        axis.line.y = element_blank(),
              axis.title.x = element_blank()) +
  labs(title = "t-fordelning",
  subtitle= "df = 10, 95% av area under kurven",
       x = "Value") +
  
  geom_point(data = samp, aes(x = samp, y = 0), 
             size = 2, 
             shape = 20,
             color = "steelblue") +

  annotate("segment", 
           x = -qt(0.975, df = 9), xend = qt(0.975, df = 9), y = 0.03, yend = 0.03, 
           color = "black") +
    annotate("point", 
           x = 0, y = 0.03, shape = 21,size = 4, fill = "steelblue") 




## Calculate confidence intervals using t and z distributions
## and compare them
set.seed(101)
N <- 2000

results10 <- data.frame(z_lwr = numeric(N), 
                      z_upr = numeric(N),
                      t_lwr = numeric(N),
                      t_upr = numeric(N), 
                      mean = numeric(N))

results50 <- data.frame(z_lwr = numeric(N), 
                      z_upr = numeric(N),
                      t_lwr = numeric(N),
                      t_upr = numeric(N), 
                      mean = numeric(N))


for(i in 1:N) {
  

  sample <- rnorm(10,3485,587) 
  mean <- mean(sample)
  sd <- sd(sample)
  n <- length(sample)
  z <- qnorm(0.975)
  t <- qt(0.975, df = n-1)
  
  results10[i,1] <- mean - z * sd/sqrt(n)
  results10[i,2] <- mean + z * sd/sqrt(n)
  results10[i,3] <- mean - t * sd/sqrt(n)
  results10[i,4] <- mean + t * sd/sqrt(n)
  results10[i,5] <- mean 
  results10$n <- n
  
  sample <- rnorm(50,3485,587) 
  mean <- mean(sample)
  sd <- sd(sample)
  n <- length(sample)
  z <- qnorm(0.975)
  t <- qt(0.975, df = n-1)
  
  results50[i,1] <- mean - z * sd/sqrt(n)
  results50[i,2] <- mean + z * sd/sqrt(n)
  results50[i,3] <- mean - t * sd/sqrt(n)
  results50[i,4] <- mean + t * sd/sqrt(n)
  results50[i,5] <- mean 
  results50$n <- n 
  
  }

  
errorrates <- bind_rows(results10, results50) %>%
  mutate(z_includes = ifelse(3485 > z_lwr & 3485 < z_upr, 1, 0),
         t_includes = ifelse(3485 > t_lwr & 3485 < t_upr, 1, 0)) %>%
  reframe(.by = n, 
            z_includes = round(100 * (1 - sum(z_includes)/N),1),
            t_includes = round(100 * (1 - sum(t_includes)/N),1)) 


p3 <- bind_rows(results10, results50) %>%
  group_by(n) %>%
  slice_sample(n = 50) %>%
  mutate(int = row_number()) %>%
  pivot_longer(cols = c(z_lwr, z_upr, t_lwr, t_upr), 
               names_to = "interval", 
               values_to = "val") %>%
  separate(interval, into = c("type", "side"), sep = "_") %>%
  pivot_wider(names_from = side, values_from = val) %>%
  ungroup() %>%
  mutate(includes = ifelse(3485 > lwr & 3485 < upr, "in", "out"),
        error_rate = if_else(type == "z" & n == 10, errorrates[1,2],
                             if_else(type == "z" & n == 50,errorrates[2,2],
                                     if_else(type == "t" & n == 10, errorrates[1,3],
                                             errorrates[2,3]))),
         type = if_else(type == "z", "Normal", "T"),

         n = paste("n =", n, ", feilrate = ", error_rate, "%") ) %>%
        
  
  
  ggplot(aes(x = int, y = mean, 
             color = type, alpha = includes)) +
  
    geom_hline(yintercept = 3485, linetype = 2) +
  
  geom_errorbar(aes(ymin = lwr, ymax = upr), 
                position = position_dodge(width = 0.5), 
                width = 0) +
  
  scale_alpha_manual(values = c(0.2,1), ## exclude from legend
                     guide = "none") +
  scale_color_manual(values = c("orchid", "steelblue")) +
  
  geom_point(position = position_dodge(width = 0.5)) + 
  facet_wrap(~ n, ncol = 1) +
  theme_classic() + 
  theme(strip.background = element_rect(color = "white"), 
        legend.position = "bottom") + 
  labs(x = "Utvalg", 
       y = "Gjennomsnitt (gram) 95% konfidensintervall", 
       color = "Sannsynlighetsfordeling")
  
  
plot_grid(NULL, 
          plot_grid(p1, p2, ncol = 2, rel_widths = c(1,1)), 
          p3, nrow = 3, rel_heights = c(0.05,0.4,1)) +
  # add labels
  draw_plot_label(label = c("A", "B", "C"), 
                  x = c(0.01, 0.48, 0.01),
                  y = c(1, 1, 0.7)) 

```

I din mer avanserte statistikkbok finner du kanskje følgende formel for et konfidensintervall for gjennomsnittet basert på t-fordelingen:

$$\bar{x} \pm t_{\text{df}=9,\alpha/2} \times \frac{s}{\sqrt{n}}$$
$t_{\text{df}=9,\alpha/2}$ er faktoren som angir hvor mange standardfeil vi må bevege oss fra gjennomsnittet for å finne konfidensintervallets grenser, gitt at vi bestemmer oss for $\alpha$. For et 95% konfidensintervall med antall frihetsgrader satt til 10 er faktoren `r round(qt(0.975, df=9), 3)`.

Konfidensintervaller er altså en måte å uttrykke usikkerhet på, men det er viktig å huske hva de beskriver. Et 95% konfidensintervall sier at ved repeterte utvalg/eksperimenter vil 95% av konfidensintervallene, som det observerte intervallet kommer fra, fange inn den parameter vi ønsker å estimere fra populasjonen (populasjonsparameteren). Men vi vet ikke om vårt intervall faktisk gjør det, vi kjenner bare til sannsynligheten for at vi gjør det i det lange løp.

## Hypotesetesting og p-verdier
I statistikken har vi mulighet å teste hvor kompatible våre data er med en gitt hypotese. Vi kan formulere en hypotese for kontinuerlig data gjennom å velge et tall som vi tester mot. Den frekventistiske statistikken bruker nullhypoteser og rundt denne hypotesen bygger vi opp en estimert utvalgsfordeling. Vi kan nå besvare spørsmålet: Gitt att nullhypotesen er sann, hvor sannsynlig er det at vi får et resultat så ekstremt som det vi observerer, eller enda mer ekstremt?

Denne definisjonen er dessverre ikke helt intuitiv, vi lager et eksempel under for å bedre forstå den. La oss si at vi gjennomfører et forsøk hvor vi studerer effekten av fysisk aktivitet på blodtrykk. De rekrutterte deltakerne (n=50) som i utgangpunkt har høyt blodtrykk fordeles tilfeldig (randomisert) til to grupper. Gruppe A får ingen retningslinjer for fysisk aktivitet, gruppe B får oppfølging fra en personlig trener. Etter en intervensjonsperiode tester vi blodtrykket (@fig-hypotesedata).

Fra studien er det mulig å formulere to hypoteser, en nullhypotese ($\text{H}_0$) sier at det ikke er noen forskjell mellom behandlingene. Den alternative hypotesen ($\text{H}_0$) sier derimot at det er en forskjell i blodtrykk mellom behandlingene. Hypotesene omhandler populasjonen, vi ønsker altså å si noe om hvordan behandlingen virker, ikke bare i utvalget, men også i populasjonen som deltakerne kommer fra. Filosofiske argumenter gir at det er vanskelig å bevise en hypotese men enklere å motbevise den. I statistikken prøver vi derfor vanligvis å motbevise (falsifisere) nullhypotesen, og vi sier at vi *tester mot den*. Vi setter opp testet sånn at om testresultatet er tilstrekkelig ekstremt gitt at nullhypotesen er sann så avkrefter vi den, eller finner den mindre trolig enn en alternative hypotese.



$$\text{H}_0: \mu_1 = \mu_2$$

$$\text{H}_A: \mu_1 \neq \mu_2$$

```{r}
#| echo: false
#| message: FALSE
#| warning: FALSE
#| fig-width: 4
#| fig-height: 3
#| label: fig-hypotesedata
#| fig-cap: "Data fra en hypotetisk studie hvor deltakere blir randomisert til to grupper. Gruppe A får ingen intervensjon, gruppe B får oppfølging fra en personlig trener. Systolisk blodtrykk er målt etter intervensjonen."


### Create data sets of blood pressure 

set.seed(1)
A <- rnorm(25, 139, 12)
B <- rnorm(25, 134, 12)



data.frame(A, B) %>%
  pivot_longer(cols = c(A, B), 
               names_to = "Gruppe", 
               values_to = "Blodtrykk") %>%
  ggplot(aes(x = Gruppe,y = Blodtrykk, fill = Gruppe)) + 
  geom_point(shape = 21, position = position_jitter(width = 0.05)) +

  theme_classic() + 
  
    theme(legend.position = "none") +
  labs(x = "Gruppe", y = "Systolisk blodtrykk (mmHg)") 


```

Datane vi samlet inn forteller at forskjellen mellom gruppene i systolisk blodtrykk etter intervensjonen er `r round(mean(A) - mean(B), 1)` mmHg. Hvis nullhypotesen er sann, hvor usannsynlig er det observerte resultatet? For å etterligne en nullhypotese skaper vi en kunstig utvalgsfordeling. Denne fordelingen lager vi gjennom å gi gruppetilhørighet til våre observasjoner helt tilfeldig, 10 000 ganger. Vi trekker altså tilfeldig deltakere fra utvalget og plasserer de i to grupper, la oss si, *"a"* og *"b"*. Hver gang beregner vi et gjennomsnitt mellom gruppene som nå er en blanding av individer fra de faktiske intervensjonsgruppene. Denne fordelingen ligner hva vi kunne forvente oss vi tok flere utvalg fra en populasjon hvor nullhypotesen faktisk var sann. Gjennomsnittene samler vi opp og så beregner vi hvor mange gjennomsnitt som er så ekstreme eller enda mer ekstreme sammenlignet med det observerte gjennomsnittet fra intervensjonen. Vi sammenligner altså resultatet fra intervensjonen med gjennomsnitt som er mulige hvis effekten av tilfeldigheter er større enn intervensjonen. Denne teknikken kalles for permutasjonstest og resultatet finner vi i @fig-permutasjonstest.


```{r}
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| fig-width: 4
#| fig-height: 3
#| label: fig-permutasjonstest
#| fig-cap: "En permutasjonstest for å sammenligne to grupper. Det observerte gjennomsnittet fra intervensjonen er markert med en rød linje. Histogrammet viser fordelingen av gjennomsnitt fra 10 000 permutasjoner hvor observert data blir tilfeldig inndelt i to grupper og en differens beregnes."


N <- 10000


diffs <- vector()
for(i in 1:N) {
  
 dat <-  data.frame(val = c(A, B)) %>%
    mutate(group = sample(rep(c("a", "b"), each = 25), 50, replace = FALSE)) 
  
  m <- lm(val ~ group, data = dat)
  diffs[i] <- coef(m)[2]
  
}


## Number of more extreme values

oneside <- sum(diffs > mean(A) - mean(B), na.rm = TRUE) / N

twoside <- sum(abs(diffs) > abs(mean(A) - mean(B)), na.rm = TRUE) / N



data.frame(diffs) %>%
  ggplot(aes(x = diffs)) + 
  
  geom_histogram(fill = "lightblue", 
                 color = "gray23") + 
  
  geom_vline(xintercept = mean(A) - mean(B), color = "red") + 
  annotate("text", x = mean(A) - mean(B) - 0.9, y = 500, label = "Observert forskjell", color = "red", 
           angle =90) +
  
  
  theme_classic() + 
  labs(x = "Forskjell i gjennomsnitt", y = "Antall permutasjoner")




```

Det viser seg at bare `r oneside`% av gjennomsnittene i den permuterte fordelingen er mer ekstreme enn det gjennomsnitt vi fikk fra intervensjonen. Er dette nok for å forkaste nullhypotesen? La oss si at vi ønsker å sammenligne resultatene fra intervensjonen med alle ekstreme resultater i permuteringstesten, dette betyr at vi også vil ta med de mer ekstreme negative forskjellene i beregningen. Det viser seg at `r twoside`% av gjennomsnittene i den permuterte fordelingen er mer ekstreme enn det observerte gjennomsnittet når vi inkluderer gjennomsnitt som er større enn `r round(mean(A) - mean(B), 1)` mmHg og mindre enn -`r round(mean(A) - mean(B), 1)` mmHg (@fig-permutasjonstest2).

```{r}
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| fig-width: 4
#| fig-height: 3
#| label: fig-permutasjonstest2
#| fig-cap: "En tosidig permutasjonstest for å sammenligne to grupper. En tosidig test sammenligner det observerte gjennomsnittet med alle ekstreme gjennomsnitt i permutasjonsfordelingen. Vi besvarer spørsmålet, hvor mange teoretiske studier gir oss så ekstreme resultater som det observerte, gitt at nullhypotesen er sann?"




data.frame(diffs) %>%
  ggplot(aes(x = diffs)) + 
  
  geom_histogram(fill = "lightblue", 
                 color = "gray23") + 
  
  geom_vline(xintercept = mean(A) - mean(B), color = "red") + 
    geom_vline(xintercept = -(mean(A) - mean(B)), color = "red") + 
  annotate("text", x = mean(A) - mean(B) - 0.9, y = 500, label = "Observert forskjell", color = "red", 
           angle =90) +
  
  
  theme_classic() + 
  labs(x = "Forskjell i gjennomsnitt", y = "Antall permutasjoner")




```


Vi har nå beregnet to *p*-verdier. Den første varianten er p-verdien som tilsvarer en ensidig test, vi sammenligner det observerte gjennomsnittet med gjennomsnittene i permutasjonsfordelingen som er større enn det observerte gjennomsnittet. Den andre varianten er en tosidig test, vi sammenligner det observerte gjennomsnittet med alle ekstreme gjennomsnitt i permutasjonsfordelingen. 

Så hva bruker vi *p*-verdien til? I frekvensitisks statistikk setter vi vanligvis en grense for hva som er *statistisk signifikant*. Vi sier at en forskjell er signifikant hvis den er mer ekstrem enn vår grense. Denne grensen settes ofte til 5%.[^sig] La oss si at nullhyotesen faktisk er sann, det finnes ingen forskjell mellom behandlingene i populasjonen. I denne situasjonen vil vi fortsatt ha mulighet å observere ekstreme forskjeller i et utvalg. Når vi setter grensen for et signifikant resultat til 5% aksepterer vi at vå prosedyre tar feil i 5% av tilfellene. Vi vet ikke om resultatet vi har observert er et slik resultat, men vi kan være sikre på at i det lange løp så vil vi bare ta fel 1 gang av 20. 

Hvis vi bruker en signifikansnivå på 5% i vårt eksempel hvor p-verdien den tosidige testen er `r twoside`% så vil vi alstå si at resultatet er signifikant. Vi konkluderer med at resultatet er så pass ekstremt at det tilhører de 5% mest ekstreme resultatene som hadde blitt observert hvis nullhypotesen er sann. Vi finner altså at vi har svak støtte for nullhypotesen og vi forkaster den til fordel for den alternative hypotesen.

Det at forkaste nullhypotesen, hvis den faktisk er sann, kalles for å gjøre en type 1 feil. Ved hjelp av signifikansnivået spesifiserer vi hvor ofte vi aksepterer å gjøre en type 1 feil. Når vi setter grensen til 5% vil vi gjøre feil i 1 studie av 20. Hvis vi setter grensen til 10% vil vi gjøre feil i 1 av 10 studier, og hvis vi setter grensen til 1% (p = 0.01), vil vi si at nullhypotesen er falsk til tross for at den er sann i 1 av 100 studier. Det å sette en grense for p-verdien handler altså om å kontrollere feilraten i det lange løp.

[^sig]: Hvorfor 5%? Det korte svaret på dette spørsmålet er, tradisjon. Vanligvis sies det at statistikeren [Ronald Fisher](https://no.wikipedia.org/wiki/Ronald_Fisher) etablerte 5% som grense for statistisk signifikant, men dette var trolig ikke nytt på 1920-tallet [@cowles1982].

### Parametrisk og ikke-parametrisk statistikk
I eksemplet over brukte vi en permutasjonstest for å bestemme om den forskjell vi observerte mellom gruppene var ekstrem nok til å forkaste hypotesen at det ikke er en forskjell mellom behandlingen i populasjonen. Denne testen er en så kalt ikke-parametrisk test, den bruker ikke en teoretisk fordeling for å beregne testresultatet (p-verdien). Hvis vi er villige til å anta at dataene er normalfordelt og at variasjonen er lik i begge grupper[^antag] så kan vi bruke en test for å sammenligne to gjennomsnitt som bruker en sannsynlighetsfordeling for å lage en modell over utvalgsfordelingen. Denne testen kalles for en *t*-test.

*t*-testen bruker t-fordelingen for å lage en modell over utvalgsfordelingen, noe som vises i @fig-permutasjonstest3. Vi ser at t-fordelingen i stort sett gir det samme resultatet som metoden som bruker permuteringer, men her bruker vi bare det ene utvalget og estimerer utvalgsfordelingn. Akkurat som for beregningen av konfidensintervallet over bruker vi en standardfeil for å bestemme vidden på utvalgsfordelingen, men vi setter sentrum til 0, altså vår nullhypotese. Standardfeilen for en *t*-test av to gjennomsnitt beregnes fra variasjonen i dataene og antallet observasjoner (se @navarro_learning_2018 for detaljer, [Avsnitt 11.3](https://davidfoxcroft.github.io/lsj-book/11-Comparing-two-means.html#sec-the-independent-samples-t-test-student-test)).    


[^antag]: Antagelser for parametrisk test av gjennomsnitt i to uavhengige grupper, også kalt en *t*-test, er at dataene er normalfordelt, noe som vil gi normalfordeling i en utvalgsfordeling. Da utvalgsfordelingen er helt hypotetisk utvidder man denne antaglsen til å inkluderer dataene som man observerer. En annen antagelse er at dataene er uavhengige, dette betyr at det ikke finnes noen slektskap i dataene, for eksempel ikke flere datapunkter fra det samme individet. Til sist antar vi at variasjonen er lik i begge gruppene, stengt tatt at dette er fallet i populasjonen. Det finnes måter å "teste" disse antagelsene på (se [11.4.3 i @navarro_learning_2018](https://davidfoxcroft.github.io/lsj-book/11-Comparing-two-means.html), men disse testene er ikke gode for å oppdage avvikelser fra antagleser i små utvalg. Dette betyr at antaglser er noe vi i stor grad må akseptere uten å kunne teste dem.



```{r}
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| fig-width: 6
#| fig-height: 3
#| label: fig-permutasjonstest3
#| fig-cap: "Resultater fra permuteringstesten sammenlignet med en t-test. Histogrammet viser utvalgsfordelingen av gjennomsnitt fra 10000 permuteringer og den lila kurven viser utvalgsfordelingen av gjennomsnitt basert på en t-fordeling. De skyggelagte områdene under t-fordelingen viser hvor p-verdien er mindre enn 0.05, hvert område fanger 2.5% av area under kurven." 


# Borrowing a solution from https://stackoverflow.com/questions/46848998/superimposing-asymmetric-t-distribution-using-ggplot2
# Define a Student t distribution with shape (nu) and location (mu)

dt2 <- function(x, mu, nu, df, ncp) {
  dt((x-mu)/nu,df,ncp)/nu
}

test_stat <- t.test(A, B, var.equal = TRUE)


dt2_limit_lower <- function(x) {
    y <- dt2(x, mu = 0, nu = test_stat$stderr, df = test_stat$parameter, ncp = 0)
    y[x < -qt(0.999, df = test_stat$parameter) * test_stat$stderr  |  x > -qt(0.975, df = test_stat$parameter) * test_stat$stderr] <- NA
    return(y)
}

dt2_limit_upper <- function(x) {
    y <- dt2(x, mu = 0, nu = test_stat$stderr, df = test_stat$parameter, ncp = 0)
    y[x < qt(0.975, df = test_stat$parameter) * test_stat$stderr  |  x > qt(0.999, df = test_stat$parameter) * test_stat$stderr] <- NA
    return(y)
}



data.frame(diffs) %>%
  ggplot(aes(x = diffs)) + 
  
  geom_histogram(fill = "steelblue",
                 alpha = 0.3,
                 aes(y = after_stat(density)),
                 color = "gray23") + 
  
  stat_function(fun = dt2, args = list(mu = 0, nu = test_stat$stderr, df = 48, ncp = 0), 
                color = "purple", linewidth = 1.5) +
  
  stat_function(fun = dt2_limit_lower, geom = "area", fill = "orchid", alpha = 0.8) +
  stat_function(fun = dt2_limit_upper, geom = "area", fill = "orchid", alpha = 0.8) +
  
    annotate("curve", 
           curvature = 0.1,
           x = -8,
           xend = -7, 
           y = 0.05, yend = 0.018, arrow = arrow(type = "closed", length = unit(0.2, "cm")), 
           color = "orchid") +
  annotate("text", label = "2.5% av arealet\nunder kurven", 
           x = -8, y = 0.07, color = "orchid", size = 4) +
  
  
      annotate("curve", 
           curvature = -0.1,
           x = 8,
           xend = 7, 
           y = 0.05, yend = 0.018, arrow = arrow(type = "closed", length = unit(0.2, "cm")), 
           color = "orchid") +
  annotate("text", label = "2.5% av arealet\nunder kurven", 
           x = 9.5, y = 0.065, color = "orchid", size = 4) +
  
        annotate("curve", 
           curvature = -0.1,
           x = -4,
           xend = -3, 
           y = 0.12, yend = 0.11, arrow = arrow(type = "closed", length = unit(0.2, "cm")), 
           color = "purple") +
  annotate("text", label = "Utvalgsfordeling\nav gjennomsnitt\n(t-fordeling)",
           x = -6, y = 0.13, color = "purple", size = 4) +
  
  
          annotate("curve", 
           curvature = 0.1,
           x = 3.5,
           xend = 2.5, 
           y = 0.12, yend = 0.11, arrow = arrow(type = "closed", length = unit(0.2, "cm")), 
           color = "steelblue") +
  annotate("text", label = "Utvalgsfordeling\nav gjennomsnitt\n(permuteringer)",
           x = 6, y = 0.13, color = "steelblue", size = 4) +
  
  
  
  annotate("segment", 
           x = mean(A) - mean(B),
           xend = mean(A) - mean(B), 
           y = 0.075, yend = 0.018, arrow = arrow(type = "closed", length = unit(0.2, "cm")), 
           color = "red") +
  annotate("text", label = "Observert\nresultat", 
           x = mean(A) - mean(B) - 0.5, y = 0.09, color = "red", size = 4) +
  
  theme_classic() +
  labs(x = "Forskjell i gjennomsnitt") + 
    theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(), 
        axis.title.y = element_blank(),
        axis.line.y = element_blank()) 




```

```{r}
#| echo: false


tcritical <- qt(0.975, df = test_stat$parameter) * test_stat$stderr
permcritical  <- round(quantile( diffs, c(0.025, 0.975)), 2)



```



Basert på *t*-fordelingen kan vi beregne en kritisk verdi, verdien på forskjellen mellom grupper som kreves for å tolke resultatet som statistisk signifikant. Ved bruk av en tosidig *t*-test er den kritiske verdien i forskjellen mellom gruppene &pm; `r round(tcritical, 2)` mmHg. Hvis forskjellen mellom gruppene er mer ekstrem enn denne verdien, noe som tilsvarer en *p*-verdi < 0.05 vil vi forkaste nullhypotesen. For permuteringstesten er de samme kritiske verdiene `r permcritical[1]` og `r permcritical[2]`. 

Vi har nå illustrert at vi kan tenke på en p-verdi som en andel av repeterte utvalg under nullhypotesen. Vi setter en grense for hvor mange repeterte utvalg vi aksepterer å gjøre en type 1 feil, det å forkaste nullhypotesen til tross for at den er sann. Når vi beregner *p*-verdien beregner vi en proporsjon, andelen av repeterte utvalg som gir oss et resultat som er mer ekstremt enn det vi observerte.

## Type 2 feil, statistisk styrke og utvalgsstørrelser
Så langt har vi konstatert at vi kan gjøre en type 1 feil ved å forkaste nullhypotesen til tross for at den er riktig. Den frekventisktiske statistikken er opptatt av å kontrollere denne feilen, vi ønsker statistiske tester som har en gitt feilrate i det lange løp (over flere lignende, uavhengige studier). I tillegg til en type 1 feil kan vi også gjøre en annen feil. Ved å *ikke* forkaste nullhypotesen til tross for at ikke er riktig gjør vi en type 2 feil. Denne feilen krever litt mer arbeid fra oss som skal analysere dataene. Vi kan sette opp de to typene feil i en tabell som under.


|Nullhypotesen er... |Sann |Falsk|
| --- | --- |---|
|**Forkasted** |<span style="color:red">Type-1 feil</span>|Riktig avgjørelse|
|**Ikke forkasted**|Riktig avgjørelse|<span style="color:red">Type-2 feil</span>|

I et scenario med to grupper som vi ønsker å sammenligne har vi formulert en nullhypotese som sier at det ikke finnes en forskjell mellom gruppene på populasjonsnivå. Før vi innhenter data formulerer vi også en alternativ hypotese. Vi lager denne alternative hypotesen basert på noen fakta vi allerede har om problemet. La oss ta fysisk aktivitet og blodtrykk som eksempel igjen.

En forandring i systolisk blodtrykk etter en behandling så stor som 5-10 mmHg kan sies være den minste forskjellen som er klinisk betydningsfull. Her kan vi argumentere for at en senkning av blodtrykk med 5-10 mmHg kreves for at en individ skal oppleve helsefordeler med behandlingen. Vi bruker 10 mmHg for å etablere en alternativ hypotese til nullhypotesen. Vi kan også formulere det som at 10 mmHg er den minste gjennomsnittlige senkingen av blodtrykk som vi er intresserte i å finne. Vi ønsker nå en statistisk test som oppdager denne forskjellen mellom to grupper, om den faktisk finnes. Evnen til en statistisk test å forkaste nullhypotesen til fordel for den alternative hypotesen kalles for statistisk styrke. Den statistiske styrken defineres som en minus den forventede raten med hvilken vi gjør type 2 feil (ofte sier vi $1-\beta$, hvor $\beta$ er grensen vi setter for å gjøre en type 2 feil). Det som påvirker den statistiske styrken er størrelsen på effekten (eller forskjellen mellom gruppen som i vårt eksempel), og størrelsen på utvalget. 

I populasjonen som vi ønsker å undersøke er den gjennomsnittlige systoliske blotrykken 135 mmHg med en standardavvik på 20 mmHg. Vår alternative hypotese er at fysisk aktivitet senker blodtrykket med 10 mmHg. Disse tallene kan vi bruke for å beregne en standardisert effektstørrelse ($d$), denne er

$$d = \frac{H_a}{SD} = \frac{10}{20} = 0.5$$. 

En standardisert effektstørrelse er en måte å beskrive en effekt i termer av variasjonen. Hvor stor er effekten i forhold til den gjennomsnittlige variasjonen i populasjonen? Neste steg blir å bestemme hvilken statistisk styrke og hvor stor risiko for type 1 feil vi ønsker i testen. Her kan vi bruke en argumentasjon som går ut på at en type 1 feil er alvorligere enn type 2 feil. La oss si 4 ganger alvorligere, hvis vi ikke ønsker å gjøre en type 1 feil i mer enn 5% av repeterte studier kan vi leve med risikoen å gjøre en type 2 feil som er $5\% \times 4 = 20\%$.

Vi har nu mulighet for å beregne antallet deltakere som kreves for å oppnå en statistisk styrke på 80%. Til denne beregning bruker vi følgende tall:

| | |
|--- | ---|
|Effektstørrelse | 0.5|
|Risiko for type 1 feil ($\alpha$)| 5%|
|Risiko for type 2 feil ($\beta$)| 20%|
|Statistisk styrke ($1-\beta$)| 0.8|

Ditt statistikkprogram (for eksempel Jamovi) kan beregne antallet deltakere som kreves for å oppnå en gitt statistisk styrke, resultatet fra en slik analyse kan beskrives i en figur hvor to hypoteser er representert som utvalgsfordelinger. Disse utvalgsfordelingene tilsvarer de to hypotesene gitt de parametre vi gir til programmet (se tabellen over). Den alternative hypotesen er sentrert over 0.5 og spredningen tilpasses vårt ønske om feilraten for type 1 og 2-feil. Ettersom spredningen i en utvalgsfordeling er proporsjonal til antallet observasjoner (deltakere i studien) vil dette resultatet kreve et gitt antall deltakere til vårt eksperiment.

```{r}
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| fig-width: 7
#| fig-height: 5
#| label: fig-power
#| fig-cap: "Resultatet av en analyse av statistisk styrke (power analysis) for en test forskjell mellom to gjennomsnitt. Med en effektsstørrelse på 0.5 gir 64 deltakere i hver gruppe en statistisk styrke på 80%. Den statistiske styrken er area under kurven i den alternative hypotesen som ikke overlapper med den regionen under nullhypotesen hvor vi ikke forkaster nullhypotesen(!). Vi får en type 2 feil når vi ikke forkaster nullhypotesen selv om den er falsk (og den alternative hypotesen er sann, grønn kurve)." 


# A figure showing the power analysis of a two-sample t-test
# The x-axis shows the standardized effects size (d) 
# To plot the sampling distributions of each hypothesis the 
# standard error of the two sample t-test is calculated

# calculate power and the standard error for the sampling distribution
library(pwr)
pwrtest <- pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = "two.sample")

# Number of participants in each group
n <- round(pwrtest$n)
# The standard error of the sampling distribution
sigma_se <- sqrt(((n-1) * 1^2 + (n-1) * 1^2)/ ((n + n -2)) * ((1/n) + (1/n))) 

# Function for shading the lower critical region of the null hypothesis
dt2_critical <- function(x, region = "lwr", n, se) {
  # Define the sampling distribution of the null hypothesis  
  y <- dt2(x, mu = 0, nu = sigma_se, df = n-2, ncp = 0)
    if(region == "lwr") {
        y[x < -qt(0.99999, df = n-2) * se  |  x > -qt(0.975, df = n-2) * se] <- NA
    } 
    if(region == "upr") {
        y[x > qt(0.99999, df = n-2) * se  |  x < qt(0.975, df = n-2) * se] <- NA
    } 
    return(y)
}


dt2_limit_lower_ha <- function(x) {
    y <- dt2(x, mu = 0.5, nu = sigma_se, df = n-2, ncp = 0)
    y[x < 0.5 - qt(0.99999, df = n-2) * sigma_se  |  x > 0.5 - qt(0.8, df = n-2) * sigma_se] <- NA
    return(y)
}









data.frame(x = c(-1, 1)) %>%
  ggplot(aes(x = x)) + 
  

  
  stat_function(geom = "area", 
                alpha = 0.5,
                color = "orchid", linewidth = 0,
                fill = "orchid",
                fun = dt2_critical, 
                n = 1000,
                args = list(region = "lwr", se = sigma_se, n = n)) +
  
  stat_function(geom = "area", 
                alpha = 0.5,
                n = 1000,
                color = "orchid", linewidth = 0,
                fill = "orchid",
                fun = dt2_critical, 
                args = list(region = "upr", se = sigma_se, n = n)) +
  
  stat_function(fun = dt2, geom= "area", args = list(mu = 0, nu = sigma_se, df = n-2, ncp = 0), 
                  fill = "orchid", alpha = 0.2,
                  n = 1000,
                  
                color = "orchid", linewidth = 0) +
  
  
  
  
  stat_function(fun = dt2,
                geom = "area",
                fill = "lightgreen",
                alpha = 0.2,
                n = 1000,
                args = list(mu = 0.5, nu =sigma_se, df = n-2, ncp = 0), 
                color = "lightgreen", linewidth = 0) +
  scale_x_continuous(limits = c(-1, 1.2), 
                    breaks = seq(from = -1, to = 1, by = 0.5)) +

    stat_function(geom = "area",
                fill = "darkgreen",
                alpha = 0.2,
                 n = 1000,
                fun = dt2_limit_lower_ha) +
  
  
  theme_classic() +

  
  labs(x = "Standardisert effektst&#248;rrelse (d)") +
  
    theme(axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.title.y = element_blank(),
        axis.line.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_markdown(size = 12),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        plot.title = element_text(size = 14),
        legend.position = "none") +
  
 # geom_vline(xintercept = qt(0.975, df = n-2) * sigma_se, linetype = "dashed", color = "black") +
## Add text to the plot  
  annotate("text", 
           x = -0.5, 
           y = 1.5, 
           label = "Utvalgsfordeling \nunder nullhypotese", 
           size = 5, 
           color = "orchid") +
  
  annotate("curve",
           x = -0.5,
           y = 1.35,
           xend = -0.35,
           yend = 1.1,
           arrow = arrow(type = "closed", length = unit(0.1, "inches")),
           color = "orchid", 
           curvature = 0.2) +
  
  annotate("text", 
           x = 0.95, 
           y = 1.7, 
           label = "Utvalgsfordeling under\nalternativ hypotese", 
           size = 5, 
           color = "darkgreen") +
  
  annotate("curve",
           x = 0.85,
           y = 1.55,
           xend = 0.75,
           yend = 1.5,
           arrow = arrow(type = "closed", length = unit(0.1, "inches")),
           color = "darkgreen", 
           curvature = -0.2) +

  annotate("text", 
           x = 0.95, 
           y = 1.05, 
           label = "Statistisk\nstyrke",
 
           size = 5, 
           color = "darkgreen") +
  
  annotate("curve",
           x = 0.9,
           y = 0.9,
           xend = 0.8,
           yend = 0.75,
           arrow = arrow(type = "closed", length = unit(0.1, "inches")),
           color = "darkgreen", 
           curvature = -0.2) +

  annotate("text", 
           x = 0.25, 
           y = 1.95, 
           label = "Type 2 feil",
 
           size = 5, 
           color = "darkgreen") +

  annotate("curve",
           x = 0.25,
           y = 1.9,
           xend = 0.3,
           yend = 1.3,
           arrow = arrow(type = "closed", length = unit(0.1, "inches")),
           color = "darkgreen", 
           curvature = 0.1) +
  
    annotate("text", 
           x = -0.6, 
           y = 0.85, 
           label = "Type 1 feil",
 
           size = 5, 
           color = "orchid") +

  annotate("curve",
           x = -0.6,
           y = 0.8,
           xend = -0.5,
           yend = 0.1,
           arrow = arrow(type = "closed", length = unit(0.1, "inches")),
           color = "orchid", 
           curvature = 0.1) +
    annotate("curve",
           x = -0.6,
           y = 0.8,
           xend = 0.4,
           yend = 0.1,
           arrow = arrow(type = "closed", length = unit(0.1, "inches")),
           color = "orchid", 
           curvature = 0.1) 
  
  
  



```

En analyse av statistisk styrke gjøres som en del i planleggingen av en studie og er en måte å argumentere for hvor mange observasjoner (deltakere) som trengs for å kunne forkaste en nullhypotese, gitt en alternativ hypotese. Når vi leser en studie som har gjennomført en statistisk styrkeanalyse kan vi et første steg se hva forskerne forventet seg av sin studie, hvor stor eller liten er effekten de forventer seg i den alternative hypotesen? 

Noen ganger vil du kanskje vare uenig i forskernes forventninger, de tror at effekten er mye større enn hva vi egentlig kan forvente. Resultatet av en slik overestimering av effekten er at få deltakere blir rekruttert til studien og at studien med stor sannsynlighet ikke vil oppdage en mindre effekt.

I mange studier finner vi ikke en analyse av statistisk styrke eller begrunnelse for hvor mange deltakere som kreves. Her kan vi som lesere bruke en statistisk styrkeanalyse for å vurdere om studien har tilstrekkelig mange deltakere for å oppdage en effekt som vi er interesserte i. Her er det viktig å huske på at vi ikke bruker resultatet fra studien som vi leser for å gjøre en styrkeanalysen. Denne ene studien er bare en observasjon i utvalgsfordelingen under null- eller den alternative hypotesen. Isteden må vi bruke informasjonen fra tidligere studier eller vår egen kunnskap om feltet for å gjøre en styrkeanalyse, altså avgjøre hva som er en trolig effekt. Dette er ikke helt intuitivt og noe som forskere også sliter med!

::: {.column-margin}
Lakens [-@lakens2022] gir en grundig gjennomgang av forskjellige måter å gi begrunnelse for en studies utvalgsstørrelse i et frekventistisk perspektiv. Utgangspunktet er at forskjellige måter å begrunne utvalgsstørrelsen på gir leseren mulighet å vurdere informasjonsverdien i studien.
:::


### Mer om effektstørrelser
Tidligere har vi snakket om sammenhenger mellom variabler og hvordan vi kan måle disse. I de fall vi ønsker å sammenligne to grupper undersøker vi om det finnes en *sammenheng mellom gruppe og den avhengige variabelen*. Det kan være enklere å si det sånn at vi ønsker å undersøke forskjellen mellom gruppene. En effekt i denne sammenhengen kan beskrives på flere måter, som en absolutt forskjell (eks. 10 mmHg), som en forskjell relativ til en utgangsverdi (eks. $10/135 = 0.074 = 7.4\%$) eller som en forskjell standardisert til standardavviket i målevariabelen ($10/20 = 0.5$). Denne standardiserte effektstørrelsen kalles også for Cohen's $d$ etter en kjent statistiker og psykolog.

En standardisert effektstørrelse kan sammenlignes mellom studier og målevariabler. Vanligvis (etter beskriving av Cohen[@cohen2013]) beskriver man en effektstørrelse som liten hvis $d = 0.2$, medium ved $d=0.5$ og stor ved $d=0.8$, men beskrivelser av effektstørrelser bør gjøres spesifikt til den konteskt hvor de brukes. En standardisert effektstørrelse kan også konverteres til forskjellige skaler. En medium Cohen's $d$ (0.5) kan for eksempel transformeres til en korrelasjonskoeffisient $r= 0.243$. Dette gjør at standardiserte effektstørrelser blir brukt i metaanalyser hvor flere studier settes sammen for å undersøke et gitt fenomen.

En standardisert effektstørrelsen har en direkte sammenheng med den observerte p-verdien og utvalgsstørrelsen. Vid en gitt utvalgsstørrelse synker p-verdien når effektstørrelsen blir større. På samme måte synker p-verdien vid en gitt effektstørrelse når utvalgsstørrelsen blir større. Dette følger av resonnementet over, en effekt, beskrevet som en ratio til variasjonen i populasjonen får en lavere p-verdi når standardfeilen og dermed spredningen i utvalgsfordelingen blir mindre. Spredningen i utvalgsfordelingen blir mindre når utvalgsstørrelsen øker. I to eksperimenter kan vi få den samme effektstørrelsen, men avhengig av utvalgsstørrelsen vil p-verdien være forskjellig. P-verdien beskytter oss fra å tolke den observerte effekten som en sann effekt i populasjonen. Basert på den grense vi setter for statistisk signifikant i forkant av studien vet vi at denne prosedyren vil gi oss en bestemt feilrate over repeterte eksperimenter.


```{r}
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| fig-width: 6
#| fig-height: 4
#| label: fig-effect-pval
#| fig-cap: "Vi kan forvente høyere statistisk styrke og dermed lavere p-verdier for en gitt effektstørrelse når utvalgsstørrelsen er større."


library(pwr)

N <- 100

df <- data.frame(n10 = rep(NA, N), n20 = rep(NA, N), n40 = rep(NA, N), n80 = rep(NA, N)) 
df2 <- data.frame(n10 = rep(NA, N), n20 = rep(NA, N), n40 = rep(NA, N), n80 = rep(NA, N))


# The standard error of the sampling distribution

pfun <- function(n, d) {
  sdp <- sqrt(((n - 1) * 1^2 + (n - 1) * 1^2) / (n + n - 2))
  md <- d * sdp
  tstat <- md / (sdp * sqrt(1/n + 1/n))
  df <- n + n - 2
  
  2*pt(-abs(tstat), df)
  
}





for(i in 1:N) {
  
  df[i,1] <- pwr.t.test(d = i/N, sig.level = 0.05, n = 10, type = "two.sample")$power
  df[i,2] <- pwr.t.test(d = i/N, sig.level = 0.05, n = 20, type = "two.sample")$power
  df[i,3] <- pwr.t.test(d = i/N, sig.level = 0.05, n = 40, type = "two.sample")$power
  df[i,4] <- pwr.t.test(d = i/N, sig.level = 0.05, n = 80, type = "two.sample")$power
  
  
  ## Calculate p-values
  df2[i,1] <- pfun(10, i/N)
  df2[i,2] <- pfun(20, i/N)
  df2[i,3] <- pfun(40, i/N)
  df2[i,4] <- pfun(80, i/N)
  

}





p1 <- df %>%
  mutate(es = 1:N/N) %>%
  pivot_longer(cols = n10:n80) %>%
  mutate(name = as.numeric(gsub("n", "", name)), 
         type = 'Statistisk styrke') %>%
  
  bind_rows(df2 %>%
  mutate(es = 1:N/N) %>%
  pivot_longer(cols = n10:n80) %>%
  mutate(name = as.numeric(gsub("n", "", name)), 
         type = "p-verdi")) %>%
  
  
  ggplot(aes(x = es, y = value, fill = factor(name))) +
  
      geom_line(aes(color = factor(name)), linewidth = 1.5) +

  labs(color = "Utvalgsst&#248;rrelse", 
       x = "Effektst&#248;rrelse", 
       y = " ")  +
  facet_wrap(~type, scales = "free_y") +
  
  theme_classic() +
  
  scale_color_manual(values = c("orchid", "darkgreen", "steelblue", "purple")) +
  
  theme(axis.title.x = element_markdown(), 
        legend.title = element_markdown(), 
        legend.position = "bottom", 
        strip.background = element_rect(fill = "white", color = "white"), 
        strip.text = element_text(size = 14, hjust = 0))
  
p1



```

## P-verdier når nullhypotesen er riktig

Vi har konstatert at vi setter grenser for de feilratene vi aksepterer for å forkaste nyllhypotesen til tross for at den er riktig (type 1 feil; $\alpha$) og for å ikke forkaste nullhypotesen til tross for at den er feil (type 2 feil; $\beta$). Disse grensene er noe vi bestemmer i forkant av en datainnsamling, dette er begrensninger vi setter på vår prosedyre. Når vi så faktisk samler inn data og analyserer den så observerer vi en p-verdi. Men hvordan ser p-verdier ut når nullhypotesen er riktig?

Vi lager et eksperiment hvor vi vet at nullhypotesen er sann. En populasjon, *A* har en gjennomsnitt på 100 og standardavvik på 10, populasjonen *B* har også en gjennomsnitt på 100 og en standardavvik på 10. Vi trekker så utvalg fra disse populasjonen og sammnligner dem med et statistisk test som tester mot nullhypotesen: det er ingen forskjell mellom populasjonene. Resultatet av hvert test er en p-verdi og vi ser på fordelingen av alle 10000 p-verdier i figuren under @fig-pval-null.

```{r}
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| fig-width: 6
#| fig-height: 3
#| label: fig-pval-null
#| fig-cap: "."

set.seed(1)
N <- 10000

pvals <- vector()
for(i in 1:N) {
   pvals[i] <- t.test(rnorm(10, 100, 10), rnorm(10, 100, 10))$p.value
 
}

data.frame(pvals) %>%
  ggplot(aes(pvals)) + 
  geom_histogram(binwidth = 0.05, boundary = 0, closed = "left", 
                 fill = "steelblue", color = "gray21") + 
  scale_x_continuous(breaks = c(0, 0.05, 0.2, 0.4, 0.6, 0.8, 0.95, 1))+
  
  labs(x = "P-verdi", y = "Antall observerte p-verdier") +
  
  theme_classic()




```

P-verdiene oppfører seg akkurat som forventet, de er hav man kaller for "uniformt" fordelt, hvilket dette fallet innebærer at vi finner omtrent like mange p-verdier mellom 0 og 0.05 som mellom for eksempel 0.95 og 1. I eksperiment hvor nullhypotesen er sann vil vi altså forkaste nullhypotesen i 5% av studiene, hvis vi bestemmer oss for å sette grensen (signifikansnivået, $\alpha$) til 0.05.

## P-verdier, konfidensintervaller og hypotesetesting.







